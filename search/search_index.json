{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to KubeNSync","text":"<p>KubeNSync is a Kubernetes operator designed to simplify resource management across namespaces and the entire cluster. Say goodbye to repetitive resource creation and hello to efficient deployment with KubeNSync!</p>"},{"location":"#what-is-kubensync","title":"What is KubeNSync?","text":"<p>KubeNSync automate the creation of Kubernetes resources using custom templates and namespace selectors. It allows to define a template for a resource and automatically apply it to multiple namespaces, ensuring consistency and reducing manual effort.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Namespace Selector: Use regex or label selectors to target specific namespaces.</li> <li>Custom Resource Templates: Define reusable templates for Kubernetes resources.</li> <li>Data Injection: Inject data from existing resources into your templates.</li> <li>Cluster-Wide Synchronization: Automatically synchronize resources to maintain the desired state.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Follow these steps to get started with KubeNSync:</p> <ol> <li> <p>Install the Operator    Deploy KubeNSync to your cluster using <code>kubectl</code>:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/install.yaml\n</code></pre> </li> <li> <p>Grant Permissions    Apply the default RBAC configuration:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/rbac.yaml\n</code></pre> </li> </ol>"},{"location":"#documentation","title":"Documentation","text":"<p>Explore the documentation to learn more:</p> <ul> <li>Get Started</li> <li>Usage</li> <li>Reference</li> <li>Examples</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/","title":"KubeNSync when using Cluster API","text":"<p>When managing multiple Kubernetes clusters, using Cluster API (CAPI) for deploying, and ArgoCD, FluxCD or similar tools for deploying applications, is a common scenario. However, managing basic cluster configuration such as CCMs, Pull Secrets, etc. can be hard to manage.</p> <p>In these cases, using tools like ClusterResourceSet (CRS) or the Helm chart provider can help, but they often require additional steps like building the Helm values, injecting secrets, and potentially depending on external tools like some kind of vault.</p>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#use-case-injecting-a-pull-secret-into-guest-clusters","title":"Use case: Injecting a pull secret into guest clusters","text":"<p>I faced this problem when deploying air-gapped clusters with a private registry. A pull secret is needed to pull images from the private registry, so there are some options available using traditional methods:</p> <ol> <li>Using ClusterResourceSet: You can create a <code>ClusterResourceSet</code> that creates the resources defined on a ConfigMap into the cluster. This is a viable option at first, but it requires to manually create the ConfigMap with the secret and rotating it for each cluster. This implies that the tool deploying the <code>ClusterResourceSet</code> must have access to the secret, which was not my case.</li> <li>Using Helm: The scenario using the Helm chart provider is similar to the previous one, but it requires to create a Helm chart with the secret and deploy it to each cluster. This is also a viable option, but, same as before, it requires to know the secret to provide it on the values.</li> <li>Configuring containerd: Another option could be to configure containerd to use the pull secret by default. This is a more complex solution, as it requires to modify the containerd configuration on each cluster. Anyway, this is not valid not only because I don't have the secret at deploy time, but also because adding the pull secret globally is not a good practice, as any pod in any namespace could download images. Also this would be limited to only one credential per registry, which is not ideal in many cases.</li> </ol> <p>In summary, I needed a generic solution that would allow me to inject arbitrary secrets into the guest clusters without knowing them at deploy time, they are already present on the host cluster and their contents might change so it needs to be synchronized. This is where KubeNSync comes into play.</p>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#prerequisites","title":"Prerequisites","text":"<p>Before we begin, ensure you have the following prerequisites:</p> <ul> <li>A Kubernetes cluster set up with Cluster API.</li> <li>KubeNSync operator installed on your host cluster.</li> <li>Guest clusters created and managed by Cluster API.</li> </ul>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#solution","title":"Solution","text":"<p>The overall idea is to use KubeNSync to synchronize a pull secret stored in the host cluster to the guest clusters, using as few pieces as possible. For achieving this, I decided to go with the ClusterResourceSet approach as it's CAPI-native and no addons are required. The steps are as follows:</p> <ol> <li>The <code>source-pull-secret</code> is a Secret in the <code>kube-system</code> namespace of the host cluster. It's referenced by a <code>ManagedResource</code>.</li> <li>The <code>ManagedResource</code> creates a <code>Secret</code> and <code>ClusterResourceSet</code> on each guest cluster namespace.</li> <li>The CAPI controller will process the <code>ClusterResourceSet</code> and get the resources defined on the <code>Secret</code>.</li> <li>The CAPI controller will create a Secret in the <code>kube-system</code> namespace of the guest cluster.</li> </ol> <pre><code>graph TD\n    subgraph Host Cluster\n        subgraph kube-system namespace\n            A[\"source-pull-secret&lt;br&gt;(Secret)\"]\n        end\n        subgraph ClusterScoped resources\n            B[\"capi-pull-secret&lt;br&gt;(ManagedResource)\"]\n        end\n        A -- \"(1) Referenced by\" --&gt; B\n        subgraph capi-cluster-1 namespace\n            B -- \"(2) Creates\" --&gt; C(\"crs-pull-secret&lt;br&gt;(Secret)\")\n            B -- \"(2) Creates\" --&gt; D(\"crs-pull-secret&lt;br&gt;(ClusterResourceSet)\")\n        end\n        D -- \"(3) Managed by\" --&gt; E[\"CAPI&lt;br&gt;Controller\"]\n    end\n    subgraph Cluster-1 Cluster\n        E -- \"(4) Creates Secret\" --&gt; F[\"pull-secret&lt;br&gt;(kube-system)\"]\n    end</code></pre> <p>Note</p> <p>The flow assumes that each guest cluster resources live in their own namespace. This is not strictly necessary, but it is a common practice when using Cluster API.</p>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#implementation","title":"Implementation","text":"<p>First of all, we need to create the <code>source-pull-secret</code> in the host cluster. This is a Secret that contains the pull secret that we want to synchronize to the guest clusters.</p> source-pull-secret.yaml<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: source-pull-secret\n  namespace: kube-system\ntype: kubernetes.io/dockerconfigjson\ndata:\n  .dockerconfigjson: &lt;base64-encoded-pull-secret&gt;\n</code></pre> <pre><code>kubectl create -f source-pull-secret.yaml\n</code></pre> <p>Now, we need to create the <code>ManagedResource</code> that will create the <code>Secret</code> and <code>ClusterResourceSet</code> on each guest cluster namespace. This is done by creating a <code>ManagedResource</code> that references the <code>source-pull-secret</code> and uses a label to identify the CAPI namespaces.</p> capi-pull-secret.yaml<pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n  name: capi-pull-secret\nspec:\n  namespaceSelector:\n    labelSelector:\n      matchLabels:\n        capi-pull-secret: \"true\"\n  template:\n    data:\n      - name: pull_secret\n        type: Secret\n        ref:\n          name: source-pull-secret\n          namespace: kube-system\n    literal: |\n      ---\n      apiVersion: v1\n      kind: Secret\n      metadata:\n        name: crs-pull-secret\n        namespace: {{ .Namespace.Name }}\n      type: addons.cluster.x-k8s.io/resource-set\n      stringData:\n        manifests.yaml: |\n          ---\n          apiVersion: v1\n          data:\n            .dockerconfigjson: '{{ index .Data.pull_secret \".dockerconfigjson\" | base64Encode }}'\n          kind: Secret\n          metadata:\n            name: pull-secret\n            namespace: kube-system\n          type: kubernetes.io/dockerconfigjson\n      ---\n      apiVersion: addons.cluster.x-k8s.io/v1beta1\n      kind: ClusterResourceSet\n      metadata:\n        name: crs-pull-secret\n        namespace: {{ .Namespace.Name }}\n      spec:\n        clusterSelector:\n          matchLabels:\n            cluster.x-k8s.io/cluster-name: {{ .Namespace.Name }}\n        resources:\n        - kind: Secret\n          name: crs-pull-secret\n        strategy: Reconcile\n</code></pre> <pre><code>kubectl create -f capi-pull-secret.yaml\n</code></pre> <p>And this is almost everything! The <code>ManagedResource</code> will create the <code>Secret</code> and <code>ClusterResourceSet</code> on each guest cluster namespace that matches the label selector. The CAPI controller will then process the <code>ClusterResourceSet</code> and create the <code>pull-secret</code> in the <code>kube-system</code> namespace of each guest cluster.</p>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#benefits","title":"Benefits","text":"<p>With this approach all of the points mentioned in the use case are covered:</p> <ul> <li>Generic solution: The <code>ManagedResource</code> can be used to synchronize any secret, not just pull secrets. Any resource for that matter.</li> <li>Secret value agnostic: Both the <code>source-pull-secret</code> Secret and <code>capi-pull-secret</code> ManagedResource can be created beforehand, so we don't need to know its value at deploy time.</li> <li>Synchronization: The <code>ManagedResource</code> will automatically synchronize the <code>source-pull-secret</code> to the <code>crs-pull-secret</code>, and CAPI will update it on the guest clusters.</li> </ul>"},{"location":"blog/2025/07/15/kubensync-when-using-cluster-api/#caveats","title":"Caveats","text":"<p>There are a few caveats to keep in mind when using this approach:</p> <ul> <li> <p>This solution assumes that the guest clusters resources are created in their own namespaces and the namespace name matches the cluster name.</p> <p>Tip</p> <p>If this is not the case, you might need to adjust both the <code>namespaceSelector</code> in the ManagedResource spec and the <code>clusterSelector</code> in the ClusterResourceSet spec to match your actual environment.</p> </li> <li> <p>The <code>ManagedResource</code> will create a <code>ClusterResourceSet</code> only in the namespaces that match the label selector. So you have to ensure that the namespaces where you want to deploy the pull secret have the label <code>capi-pull-secret: \"true\"</code>.</p> </li> </ul> <p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/","title":"Dynamically configuring RBAC for CD workflows","text":"<p>Handling RBAC (Role-Based Access Control) in Kubernetes can be challenging, especially when you need to dynamically configure permissions for CD tools that use Service Accounts (SAs) across multiple namespaces. We will explore how to configure dynamic RBAC in a real world scenario, taking ArgoCD as an example configuring the necessary permissions for its Service Account in each namespace. This approach can be adapted to other CD tools like FluxCD or Jenkins.</p>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/#the-problem","title":"The problem","text":"<p>When using tools like ArgoCD, you often setup a Cluster using some kind of credentials (or kubeconfig), usually a Service Account. Normally this Service Account is granted cluster-admin permissions, which is not recommended on production environments. This is particularly problematic when teams have access to the CD tool to deploy and manage applications themselves. Leaving cluster-admin permissions allows them to modify any resource, including core Kubernetes resources: CoreDNS, Node, Secrets, etc.</p>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/#the-ideal-solution","title":"The ideal solution","text":"<p>The ideal solution in this scenarios is to provide the CD tools with the minimum permissions required to deploy and manage only the applications and their related resources. In general, this means creating RoleBindings on the destination namespaces allowing the creation and management of resources.</p> <p>Being more specific, the desired scenario we're going to focus on is:</p> <ul> <li>A CD tool (ArgoCD for the example) is configured with a Service Account.</li> <li>The Service Account has permission to fully manage all resources in the application namespaces.</li> <li>The application namespaces are distinguished by a label, e.g. <code>namespace-type: application</code>.</li> </ul> <p>Note</p> <p>This is a simplified scenario and it assumes that someone (usually a cluster administrator or DevOps team) have already created the namespaces with the appropriate labels.</p>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/#the-implementation","title":"The implementation","text":"<p>To achieve this, only one ManagedResource is really needed, which will create a RoleBinding in each namespace with the label <code>namespace-type: application</code>. The RoleBinding will grant the necessary permissions to the Service Account used by the CD tool.</p> <p>Info</p> <p>ArgoCD could use the <code>in-cluster</code> config with its own service account, but for this example we will create a separate Service Account for the CD tool so it's extensible for external clusters too. If you want to use the in-cluster config, you can skip to point 5.</p> <ol> <li> <p>For simplicity's sake, we will use the <code>kube-system</code> namespace for the Service Account, but you can use any other namespace.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: argocd-sa\n  namespace: kube-system\n</code></pre> </li> <li> <p>For being able to import the Service Account into ArgoCD we need to create a long-lived Secret with the Service Account token:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-sa-token\n  namespace: kube-system\n  annotations:\n    kubernetes.io/service-account.name: argocd-sa\ntype: kubernetes.io/service-account-token\n</code></pre> <p>Tip</p> <p>You can use short-lived tokens, but they will require to be refreshed periodically. This could be done using <code>ManagedResources</code> to update the Secret with a new token, but for simplicity we will use a long-lived token.</p> </li> <li> <p>Now we can retrieve the token from the Secret and use it to configure the credentials in ArgoCD:</p> <pre><code>kubectl get secret argocd-sa-token -n kube-system -o jsonpath='{.data.token}' | base64 --decode\n</code></pre> </li> <li> <p>With the Service Account token register the cluster in ArgoCD:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: mycluster-secret\n  labels:\n    argocd.argoproj.io/secret-type: cluster\ntype: Opaque\nstringData:\n  name: mycluster.example.com\n  server: https://mycluster.example.com\n  config: |\n    {\n      \"bearerToken\": \"&lt;serviceaccount token&gt;\",\n      \"tlsClientConfig\": {\n        \"insecure\": false,\n        \"caData\": \"&lt;base64 encoded certificate&gt;\"\n      }\n    }\n</code></pre> <p>Info</p> <p>The <code>caData</code> is optional if the cluster uses a public CA or if you trust the cluster's certificate. Also you could set <code>insecure</code> to <code>true</code> if you want to skip certificate verification, but this is not recommended for production environments.</p> </li> <li> <p>Once the service account is configured, we have to configure some cluster-read permissions for the Service Account so ArgoCD can populate the caches:</p> <pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: argocd-read\nrules:\n  - apiGroups: [\"*\"]\n    resources: [\"*\"]\n    verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: argocd-sa-read\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: argocd-read\nsubjects:\n  - kind: ServiceAccount\n    name: argocd-sa\n    namespace: kube-system\n</code></pre> </li> <li> <p>Until this point ArgoCD can connect and read resources from the cluster, but can't deploy anything. For that, only one <code>ManagedResource</code> is needed, it will create the RoleBinding in each namespace with the label <code>namespace-type: application</code> providing <code>edit</code> permissions.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n  name: argocd-dynamic-rbac\nspec:\n  namespaceSelector:\n    labelSelector:\n      matchLabels:\n        namespace-type: application\n  template:\n    data:\n      - name: argocd-sa\n        type: ServiceAccount\n        ref:\n          name: argocd-sa\n          namespace: kube-system\n    literal: |\n      ---\n      apiVersion: rbac.authorization.k8s.io/v1\n      kind: RoleBinding\n      metadata:\n        name: argocd-rolebinding\n        namespace: {{ .Namespace.Name }}\n      subjects:\n        - kind: ServiceAccount\n          name: argocd-sa\n          namespace: kube-system\n      roleRef:\n        kind: ClusterRole\n        name: edit\n        apiGroup: rbac.authorization.k8s.io\n</code></pre> </li> </ol>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/#conclusion","title":"Conclusion","text":"<p>Using <code>ManagedResources</code> to dynamically configure RBAC for CD tools like ArgoCD is a viable and effective solution. It allows to automate the permission management allowing the tool to deploy only on the desired namespaces without granting cluster-wide admin permissions. This approach can be adapted to other CD tools like FluxCD or Jenkins, providing a flexible and secure way to manage permissions in Kubernetes clusters.</p>"},{"location":"blog/2025/08/10/dynamically-configuring-rbac-for-cd-workflows/#limitations","title":"Limitations","text":"<p>This solution assumes standard application deployments containing only basic deployments (Deployments, StatefulSets, etc.) and services. If your applications require more complex deployments, such as custom resources, cluster-scoped resources, or specific permissions, you may need to extend the <code>RoleBinding</code> or create additional <code>ManagedResources</code> to cover those cases.</p> <p>This work is licensed under a Creative Commons Attribution 4.0 International License.</p>"},{"location":"examples/","title":"Examples Overview","text":"<p>This is a collection of examples demonstrating the usage of <code>kubensync</code> in various scenarios. The examples are organized by use case and provide a basic implementation:</p> <ul> <li>Service Account</li> <li>Pull Secret</li> <li>RBAC</li> <li>ConfigMap</li> </ul>"},{"location":"examples/pull-secret/","title":"Creating a Pull Secret in All Development Namespaces","text":""},{"location":"examples/pull-secret/#use-case","title":"Use case","text":"<p>Automatic creation of a pull secret in all development namespaces. This is useful for setting up a pull secret on a per-namespace basis, especially in a multi-tenant environment, where each environment may require different credentials to access a private container registry.</p>"},{"location":"examples/pull-secret/#implementation","title":"Implementation","text":"<p>This ManagedResource (MR) will clone a pull secret from the default namespace to all namespaces that contain <code>dev-</code> in their name.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: pullsecret-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        data:\n        - name: pull_secret\n            type: Secret\n            ref:\n                name: my-pull-secret\n                namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: Secret\n            metadata:\n                name: my-pull-secret\n                namespace: {{ .Namespace.Name }}\n            type: kubernetes.io/dockerconfigjson\n            data:\n                .dockerconfigjson: '{{ index .Data.pull_secret \".dockerconfigjson\" | base64Encode }}'\n</code></pre>"},{"location":"examples/quota/","title":"Creating a ResourceQuota in All Non-Core Namespaces","text":""},{"location":"examples/quota/#use-case","title":"Use case","text":"<p>To avoid overconsumption of resources in a Kubernetes cluster, it is important to set up resource quotas in all namespace in non-core namespaces. When having a multi-tenant cluster, it is important to set up resource quotas in all namespaces, specially on non-production ones.</p>"},{"location":"examples/quota/#implementation","title":"Implementation","text":"<p>This ManagedResource (MR) will create a ResourceQuota <code>default-quota</code> in each namespace that does not start with <code>kube</code>, <code>kub</code>, or <code>k</code> (core namespaces).</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: default-quota\nspec:\n    avoidResourceUpdate: true\n    namespaceSelector:\n        regex: \"^[^k].*|k[^u].*|ku[^b].*\" # (1)!\n    template:\n        literal: |\n            ---\n            apiVersion: v1\n            kind: ResourceQuota\n            metadata:\n                name: cpu-quota\n                namespace: {{ .Namespace.Name }}\n            spec:\n                hard:\n                    cpu: \"4\"\n</code></pre> <ol> <li> <p>Warning</p> As Go regex stdlib does not support negative lookaheads the negative expressions is a bit funny. It would be <code>^(?!kube-).*</code>. </li> </ol>"},{"location":"examples/rbac-handling/","title":"Setting up RBAC permissions","text":""},{"location":"examples/rbac-handling/#use-case","title":"Use case","text":"<p>Automatic RBAC permissions applied to namespaces to give users access to specific resources. This is useful for setting up permissions in a multi-tenant environment where different teams or users need access to different resources.</p>"},{"location":"examples/rbac-handling/#implementation","title":"Implementation","text":"<p>This ManagedResource (MR) will create a Role <code>my-role</code> in each namespace that contains <code>dev-</code> in its name. The role will allow the user to get, list, and watch pods.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: rbac-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        literal: |\n            ---\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: Role\n            metadata:\n                name: my-role\n                namespace: {{ .Namespace.Name }}\n            rules:\n                - apiGroups: [\"\"]\n                  resources: [\"pods\"]\n                  verbs: [\"get\", \"list\", \"watch\"]\n</code></pre> <p>This can be extended to create a RoleBinding to bind the role to a user or group. For example, to bind the role to a user named <code>my-user</code>, you can use the following MR:</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: rbac-binding-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        literal: |\n            ---\n            apiVersion: rbac.authorization.k8s.io/v1\n            kind: RoleBinding\n            metadata:\n                name: my-role-binding\n                namespace: {{ .Namespace.Name }}\n            subjects:\n                - kind: User\n                  name: my-user\n                  apiGroup: rbac.authorization.k8s.io\n            roleRef:\n                kind: Role\n                name: my-role\n                apiGroup: rbac.authorization.k8s.io\n</code></pre>"},{"location":"examples/service-account/","title":"Creating a ServiceAccount in All Test Namespaces","text":""},{"location":"examples/service-account/#use-case","title":"Use case","text":"<p>A specific Service Account is required in all test namespaces. This is need to run test jobs in a CI/CD pipeline and the default SA is not allowed.</p>"},{"location":"examples/service-account/#implementation","title":"Implementation","text":"<p>This ManagedResource (MR) will create a Service Account <code>managed-resource-sa</code> in each namespace that contains <code>test</code> in its name.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: serviceaccount-sample\nspec:\n    namespaceSelector:\n        regex: \"test\"\n    template:\n        literal: |\n            ---\n            apiVersion: v1\n            kind: ServiceAccount\n            metadata:\n                name: managed-resource-sa\n                namespace: {{ .Namespace.Name }}\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before deploying the kubensync operator, ensure you have the following prerequisites:</p> <ul> <li>Kubernetes cluster up and running.</li> <li><code>kubectl</code> CLI tool configured to access your cluster.</li> <li><code>helm</code> CLI tool installed (if you choose to deploy using Helm).</li> <li><code>cluster-admin</code> privileges.</li> </ul>"},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#using-kubectl","title":"Using kubectl","text":"<p>Install the operator:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/install.yaml\n</code></pre> <p>Grant Permissions:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/rbac.yaml\n</code></pre> <p>Warning</p> <p>This permissions will grant the operator cluster-admin permissions. It's a good way of testing the operator, but specific permissions should be defined according to the resources it will manage in each specific case.</p>"},{"location":"getting-started/#using-helm","title":"Using Helm","text":"<p>Install the operator using the Helm chart:</p> <pre><code>helm install kubensync oci://ghcr.io/eryalito/kubensync-charts/kubensync --version 0.9.4 -n kubensync-system --create-namespace --wait\n</code></pre> <p>Helm Chart</p> <p>To get more information about the Helm chart, check the Helm Chart documentation</p>"},{"location":"getting-started/#uninstallation","title":"Uninstallation","text":""},{"location":"getting-started/#with-kubectl","title":"With kubectl","text":"<p>Delete the operator:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/install.yaml\n</code></pre> <p>Delete Permissions:</p> <pre><code>kubectl delete -f https://raw.githubusercontent.com/eryalito/kubensync-operator/master/dist/rbac.yaml\n</code></pre>"},{"location":"getting-started/#with-helm","title":"With Helm","text":"<p>Uninstall the operator using Helm:</p> <pre><code>helm uninstall kubensync -n kubensync-system\n</code></pre>"},{"location":"reference/","title":"Reference Overview","text":"<p>In this section, you will find detailed information about sections, fields, and types of the ManagedResource kind. This is a comprehensive reference guide to help you understand the structure and usage of the ManagedResource kind in KubeNSync.</p> <ul> <li>Selectors</li> <li>Template</li> <li>Template Functions</li> <li>Template Data</li> </ul>"},{"location":"reference/selectors/","title":"Namespace Selector","text":"<p>Namespaces can be selected using a regex or a label selector. The regex is applied to the namespace name, while the label selector is applied to the namespace labels. The regex and label selector are combined using an AND operation.</p>"},{"location":"reference/selectors/#regex","title":"Regex","text":"<p>The regex is applied to the namespace name. The regex is applied directly to the namespace name, so it looks for matches in the namespace name itself. For example, the regex <code>test</code> will match any namespace that contains the string <code>test</code> in its name, such as <code>test</code>, <code>test-1</code>, or <code>my-test-namespace</code>.</p> <p>The regex is a standard Go regex, so you can use any valid Go regex syntax. For example, the regex <code>^test-.*</code> will match any namespace that starts with <code>test-</code>, such as <code>test-1</code>, <code>test-2</code>, or <code>test-abc</code>.</p> <p>Tip</p> <p>As the regex is matching anything in the namespace name, it is recommended to use a regex that is as specific as possible. For example, if you want to match only namespaces that start with <code>test-</code>, you should use the regex <code>^test-.*</code> instead of just <code>test</code>. This will help avoid matching unintended namespaces.</p>"},{"location":"reference/selectors/#label-selector","title":"Label Selector","text":"<p>The label selector is applied to the namespace labels. The label selector is a standard Kubernetes label selector. For example, the label selector <code>environment=production</code> will match any namespace that has the label <code>environment</code> set to <code>production</code>.</p>"},{"location":"reference/selectors/#example","title":"Example","text":"<p>Selecting all namespaces that start with <code>shopping-</code> and have the label <code>environment=production</code>:</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    avoidResourceUpdate: false\n    namespaceSelector:\n        regex: \"^shopping-.*\"\n        labelSelector:\n            matchLabels:\n                environment: production\n    template:\n        # ...\n        # Additional YAML configuration goes here\n        # ...\n</code></pre>"},{"location":"reference/template-data/","title":"Template Data","text":"<p>Kubernetes objects can be accessed in the template using the <code>.Data</code> field. The <code>.spec.template.data</code> field contains a list of the resources that are loaded and processed by the template engine. The resources must be present in the Kubernetes cluster and must be accessible to the kubensync operator. The resources are loaded and processed in the order they are defined.</p>"},{"location":"reference/template-data/#data-field","title":"Data Field","text":"<p>The data resources are defined in the <code>spec.template.data</code> field. Each resource must have a unique name. The type can be one of the following:</p> <ul> <li><code>Secret</code>: A Kubernetes Secret resource.</li> <li><code>ConfigMap</code>: A Kubernetes ConfigMap resource.</li> <li><code>KubernetesResource</code>: A Kubernetes resource of any kind.</li> </ul> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        data:\n        - name: my_secret\n          type: Secret\n          ref:\n            name: my-secret\n            namespace: default\n        - name: my_configmap\n          type: ConfigMap\n          ref:\n            name: my-configmap\n            namespace: default\n        - name: my_resource\n          type: KubernetesResource\n          ref:\n            apiVersion: v1\n            group: \"\"\n            kind: ServiceAccount\n            name: test\n            namespace: default\n        literal: |\n            # Continue...\n</code></pre>"},{"location":"reference/template-data/#secret","title":"Secret","text":"<p>The <code>Secret</code> type is used to load a Kubernetes Secret resource. The <code>ref</code> field must contain only the name and namespace of the Secret resource. The Secret resource is loaded and processed, so the keys and values of the Secret are available in the template using the <code>.Data.&lt;name&gt;</code> syntax. The values are automatically base64 decoded, so no additional processing is needed.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        data:\n        - name: my_secret\n          type: Secret\n          ref:\n            name: my-secret\n            namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: Secret\n            metadata:\n                name: my-secret\n                namespace: {{ .Namespace.Name }}\n            type: Opaque\n            data:\n                key1: {{ index .Data.my_secret \"key1\" | base64Encode }}\n                key2: {{ index .Data.my_secret \"key2\" | base64Encode }}\n</code></pre>"},{"location":"reference/template-data/#configmap","title":"ConfigMap","text":"<p>The <code>ConfigMap</code> type is used to load a Kubernetes ConfigMap resource. The <code>ref</code> field must contain only the name and namespace of the ConfigMap resource. The ConfigMap resource is loaded and processed, so the keys and values of the ConfigMap are available in the template using the <code>.Data.&lt;name&gt;</code> syntax.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        data:\n        - name: my_configmap\n          type: ConfigMap\n          ref:\n            name: my-configmap\n            namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: ConfigMap\n            metadata:\n                name: my-configmap\n                namespace: {{ .Namespace.Name }}\n            data:\n                key1: {{ index .Data.my_configmap \"key1\" }}\n                key2: {{ index .Data.my_configmap \"key2\" }}\n</code></pre>"},{"location":"reference/template-data/#kubernetesresource","title":"KubernetesResource","text":"<p>The <code>KubernetesResource</code> type is used to load a Kubernetes resource of any kind. The <code>ref</code> field must contain the full resource definition, including the <code>apiVersion</code>, <code>group</code>, <code>kind</code>, and <code>name</code>. The resource is loaded but not processed. This means that the raw object parse into a map is available in the template using the <code>.Data.&lt;name&gt;</code> syntax.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    namespaceSelector:\n        regex: \"^dev-.*\"\n    template:\n        data:\n        - name: my_resource\n          type: KubernetesResource\n          ref:\n            apiVersion: v1\n            group: \"\"\n            kind: ServiceAccount\n            name: test\n            namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: ConfigMap\n            metadata:\n                name: my-cm\n                namespace: {{ .Namespace.Name }}\n            data:\n                serviceAccountName: '{{ .Data.my_resource.metadata.name }}'\n</code></pre> <p>Tip</p> <p>The ServiceAccount name is loaded from <code>.Data.my_resource.metadata.name</code> as the raw kubernetes object is loaded and not processed.</p>"},{"location":"reference/template-functions/","title":"Template Functions","text":"<p>The template engine used by kubensync is based on the Go template engine. It contains the default functions provided by the Go template engine, as well as some additional functions.</p>"},{"location":"reference/template-functions/#default-functions","title":"Default Functions","text":"<p>The default functions provided by the Go template engine are available in the template context. A highlight of the most commonly used functions is provided below. For a complete list of functions, see the Go template documentation.</p> <ul> <li><code>len</code>: Returns the length of a string, array, slice, or map.</li> <li><code>index</code>: Returns the element at the specified index in an array, slice, or map.</li> <li><code>slice</code>: Returns a slice of the specified array or slice.</li> <li><code>printf</code>: Formats a string using the specified format and arguments.</li> </ul>"},{"location":"reference/template-functions/#additional-functions","title":"Additional Functions","text":"<p>Sprout is included in the template engine. This library provides a set of additional functions that can be used to manipulate strings, arrays, maps and objects.</p> <p>Highlighted functions include:</p> <ul> <li><code>base64Encode</code>: Encodes a string in base64.</li> <li><code>base64Decode</code>: Decodes a base64 encoded string.</li> <li><code>fromYAML</code>: Converts a YAML string to a map.</li> <li><code>toYAML</code>: Converts a map to a YAML string.</li> <li><code>trim</code>: Trims whitespace from a string.</li> <li><code>join</code>: Joins a list of strings into a single string using the specified separator.</li> <li><code>split</code>: Splits a string into a list of strings using the specified separator.</li> <li><code>indent</code>: Indents a string by the specified number of spaces.</li> </ul> <p>Info</p> <p>Full list of functions can be found in the Sprout documentation.</p>"},{"location":"reference/template/","title":"Template","text":"<p>The templating engine used by kubensync is based on the Go template engine. It allows you to create dynamic configurations by using placeholders and functions directly integrated in the engine.</p> <p>The Go template must be provided in the <code>spec.template.literal</code> field and the output after rendering must be a list of valid YAML files that can be applied to the Kubernetes cluster, separated by <code>---</code>. The template engine will process the placeholders and functions, and generate the final YAML output.</p> <p>Custom data can be passed to the template using the <code>data</code> field. The data is a list of resources that will be injected into the template <code>.Data</code> field. The data can be any valid Kubernetes resource, such as a ConfigMap or a Secret. The data is passed to the template as a map under the <code>.Data</code> field. The key of the map is the name provided. The resource can be referenced in the template using the <code>{{ .Data.&lt;name&gt; }}</code> syntax.</p> <p>The template engine also provides a set of functions that can be used to manipulate the data. The functions are available in the template context and can be used to perform operations such as encoding, decoding, and formatting the data.</p> <p>Info</p> <p>The template engine is based on the Go template engine, so you can use any valid Go template syntax. For more information about the Go template syntax, see the Go template documentation.</p>"},{"location":"reference/template/#example","title":"Example","text":"<p>Loading and processing a secret and using it in the template:</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    avoidResourceUpdate: false\n    namespaceSelector:\n        regex: \"test\"\n    template:\n        data:\n        - name: pull_secret\n          type: Secret\n          ref:\n            name: my-pull-secret\n            namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: Secret\n            metadata:\n                name: my-pull-secret\n                namespace: {{ .Namespace.Name }}\n            type: kubernetes.io/dockerconfigjson\n            data:\n                .dockerconfigjson: '{{ index .Data.pull_secret \".dockerconfigjson\" | base64Encode }}'\n            ---\n            # Other resources can be added here\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Once the kubensync operator is installed, you can start using it by defining custom resources (CRs) that specify the resources you want to synchronize.</p>"},{"location":"usage/#managedresource","title":"ManagedResource","text":"<p>The ManagedResource kind allows users to define a template to apply for each selected namespace.</p> <pre><code>apiVersion: automation.kubensync.com/v1alpha1\nkind: ManagedResource\nmetadata:\n    name: managedresource-sample\nspec:\n    avoidResourceUpdate: false\n    namespaceSelector:\n        regex: \"test\"\n        labelSelector: # (3)!\n          matchLabels: {} # (4)!\n          matchExpressions: {} # (5)!\n    template:\n        data: # (1)!\n        - name: pull_secret # (2)!\n          type: Secret\n          ref:\n            name: my-pull-secret\n            namespace: default\n        literal: |\n            ---\n            apiVersion: v1\n            kind: Secret\n            metadata:\n                name: my-pull-secret\n                namespace: {{ .Namespace.Name }}\n            type: kubernetes.io/dockerconfigjson\n            data:\n                .dockerconfigjson: '{{ index .Data.pull_secret \".dockerconfigjson\" | base64Encode }}'\n</code></pre> <ol> <li> <p>Tip</p> You can read as many secrets or configmaps as you need, even if they are duplicates. Just keep in mind that name should be unique. </li> <li> <p>Info</p> <p>This will be the value used on the template</p> </li> <li> <p>Info</p> <p>Select namespaces based on labels.</p> </li> <li> <p>Info</p> <pre><code>DESCRIPTION:\n  matchLabels is a map of {key,value} pairs. A single {key,value} in the\n  matchLabels map is equivalent to an element of matchExpressions, whose key\n  field is \"key\", the operator is \"In\", and the values array contains only\n  \"value\". The requirements are ANDed.\n</code></pre> </li> <li> <p>Info</p> <pre><code>DESCRIPTION:\n  matchExpressions is a list of label selector requirements. The requirements\n  are ANDed.\n\n  A label selector requirement is a selector that contains values, a key, and\n  an operator that relates the key and values.\n\nFIELDS:\n  key  &lt;string&gt; -required-\n    key is the label key that the selector applies to.\n\n  operator     &lt;string&gt; -required-\n    operator represents a key's relationship to a set of values. Valid\n    operators are In, NotIn, Exists and DoesNotExist.\n\n  values       &lt;[]string&gt;\n    values is an array of string values. If the operator is In or NotIn, the\n    values array must be non-empty. If the operator is Exists or DoesNotExist,\n    the values array must be empty. This array is replaced during a strategic\n    merge patch.\n</code></pre> </li> </ol> <p>Question</p> <ul> <li><code>avoidResourceUpdate</code>: Optional field that changes the default behavior of reconciling existing resources with the desired state. If set to true only non-existing resources will be created an never updated. Default values is <code>false</code>.</li> <li><code>namespaceSelector</code>: Specifies the namespaces where you want to apply the template. You can use a regular expression (regex) to match multiple namespaces or filter them by its labels. Regex and labels are ANDed, the namespaces must match both of them to be selected. If none of them are defined, all namespaces will be selected.</li> <li><code>template</code>: Contains the YAML template that you want to apply to the selected namespaces. You can use Go template syntax to customize the resource based on the namespace.</li> <li><code>template.data</code>: Optional field that read Kubernetes resources and expose their contents to be used in the <code>template</code> under <code>.Data.&lt;name&gt;</code>.</li> </ul>"},{"location":"usage/#examples","title":"Examples","text":"<p>Check out some real-world use cases of kubensync in the examples section.</p>"},{"location":"versions/v0.8.0/","title":"Version v0.8.0","text":"<p>This is the documentation for version 0.8.0 of the project.</p>"},{"location":"versions/v0.8.0/#new-features","title":"New Features","text":"<ul> <li>Add <code>mr</code> as shortname for ManagedResources.</li> </ul>"},{"location":"versions/v0.8.0/#changes","title":"Changes","text":"<ul> <li>Developer mode is now deactivated by default. To enable it add the argument <code>-zap-devel=true</code>.</li> </ul>"},{"location":"versions/v0.8.0/#breaking-changes","title":"Breaking Changes","text":"<ul> <li> <p>Sprig functions are no longer available in the template in favor the Sprout functions.</p> <p>Tip</p> <p>All default sprout registries are enabled and available in the template. Most of the changed functions are just renamed to sprout functions. Check of the Migration Guide for more details.</p> </li> </ul>"},{"location":"versions/v0.9.0/","title":"Version v0.9.0","text":"<p>This is the documentation for version 0.9.0 of the project.</p>"},{"location":"versions/v0.9.0/#new-features","title":"New Features","text":"<ul> <li>Added a new way of deploying the operator using Helm charts. This allows for easier integration with existing Kubernetes clusters and simplifies the deployment process. Check the Installation Guide for more details.</li> </ul>"},{"location":"versions/v0.9.0/#fixes","title":"Fixes","text":"<ul> <li>Fixed scenarios where the template was not properly split by <code>---</code>. Moved to using kubernetes apimachinery method, which is the same that kubectl uses to split the template.</li> </ul>"},{"location":"versions/v0.9.0/#release","title":"Release","text":"<p>Check out the release on GitHub</p>"},{"location":"versions/v0.9.1/","title":"Version v0.9.1","text":"<p>This is the changelog for version 0.9.1.</p>"},{"location":"versions/v0.9.1/#new-features","title":"New Features","text":"<ul> <li>Blog feature has been added to the documentation site. You can now read articles and tutorials related to KubeNSync directly on the site. Check out the Blog for more details.</li> <li>License information has been updated to include a separate license for blog content. The code and documentation are under the Apache License 2.0, while blog posts are licensed under the Creative Commons Attribution 4.0 International License (CC BY 4.0).</li> </ul>"},{"location":"versions/v0.9.1/#fixes","title":"Fixes","text":"<ul> <li>Updated dependencies to fix security vulnerabilities and improve performance.</li> </ul>"},{"location":"versions/v0.9.1/#release","title":"Release","text":"<p>Check out the release on GitHub</p>"},{"location":"versions/v0.9.2/","title":"Version v0.9.2","text":"<p>This is the changelog for version 0.9.2.</p>"},{"location":"versions/v0.9.2/#fixes","title":"Fixes","text":"<ul> <li>Updated dependencies.</li> </ul>"},{"location":"versions/v0.9.2/#release","title":"Release","text":"<p>Check out the release on GitHub</p>"},{"location":"versions/v0.9.3/","title":"Version v0.9.3","text":"<p>This is the changelog for version 0.9.3.</p>"},{"location":"versions/v0.9.3/#fixes","title":"Fixes","text":"<ul> <li>Now the chart updates the tag on each release to match the application version.</li> </ul>"},{"location":"versions/v0.9.3/#release","title":"Release","text":"<p>Check out the release on GitHub</p>"},{"location":"versions/v0.9.4/","title":"Version v0.9.4","text":"<p>This is the changelog for version 0.9.4.</p>"},{"location":"versions/v0.9.4/#new","title":"New","text":"<ul> <li>Support for imagePullSecrets on the chart deployment.</li> </ul>"},{"location":"versions/v0.9.4/#release","title":"Release","text":"<p>Check out the release on GitHub</p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/use-cases/","title":"Use-cases","text":""},{"location":"blog/category/security/","title":"Security","text":""},{"location":"blog/category/cicd/","title":"CI/CD","text":""},{"location":"blog/category/cluster-api/","title":"Cluster API","text":""}]}